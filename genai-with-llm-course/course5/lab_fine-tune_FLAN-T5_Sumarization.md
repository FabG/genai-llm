# Gen AI LLM - Course 5
##  Lab - Fine-Tune FLAN-T5 to Generate More-Positive Summaries


###### Below are some key notes from [Generative AI with Large Language Models](https://www.coursera.org/learn/generative-ai-with-llms)

This labs gets our hands on RLHF
The purpose of this lab is to lower the toxicity of our instruction fine-tuned model from Lab 2, that was the output of Lab 2. That'll be the input to Lab 3 here. And we will lower the toxicity using a hate speech reward model where we want to optimize for not hate. We'll be using PPO that you learned about in the lesson, and we will wrap everything up with quantitative and then qualitative comparison of our detoxification process.


Notebook is available here:
 - [lab 3 - fully run notebook](Lab_3_fine_tune_model_to_detoxify_summaries.ipynb)
 - [lab 3 - original notebook](Lab_3_fine_tune_model_to_detoxify_summaries_ori.ipynb)

The Lab walk through is available [here](https://www.coursera.org/learn/generative-ai-with-llms/lecture/yPzI4/lab-3-walkthrough)

